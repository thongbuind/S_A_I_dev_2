import numpy as np
import json
from vncorenlp import VnCoreNLP
from pathlib import Path
import time

current_file = Path(__file__).resolve()
data_dir = current_file.parent
config_file = data_dir.parent / "config" / "config.json"
vocab_file = data_dir / "vocab.txt"
data_tokenized_dir = data_dir / "processed"  # L∆∞u folder thay v√¨ file .py
raw_dir = data_dir / "raw"

with open(config_file, 'r') as f:
    config = json.load(f)
max_seq_len = config['max_seq_len']

vocab = {}
with open(vocab_file, "r", encoding="utf-8") as f:
    for line in f:
        word, idx = line.strip().split('\t')
        vocab[word] = int(idx)
idx2word = {i: w for w, i in vocab.items()}

VNCORENLP_PATH = "/Users/thongbui.nd/vncorenlp/VnCoreNLP/VnCoreNLP-1.1.1.jar"
annotator = VnCoreNLP(VNCORENLP_PATH, annotators="wseg", max_heap_size='-Xmx2g')

def tokenize(sentence):
    word_segments = annotator.tokenize(sentence.lower())
    words = [word for segment in word_segments for word in segment]
    tokens = [vocab.get(w, vocab["[UNK]"]) for w in words]
    return tokens

def load_pretrain_dataset(file_path):
    dataset = []
    with open(file_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)
        dataset = [item.strip() for item in json_data if isinstance(item, str) and item.strip()]
    return dataset

def prepare_data(pretrain_data, vocab, max_seq_len, batch_size=50):
    X, Y, lengths = [], [], []
    max_retries = 3
    pretrain_samples = int(len(pretrain_data))

    for i in range(0, pretrain_samples, batch_size):
        batch_data = pretrain_data[i:i+batch_size]
        print(f"ƒêang x·ª≠ l√Ω batch pretrain {i//batch_size + 1}/{(pretrain_samples + batch_size - 1)//batch_size}")
        for sentence in batch_data:
            retry_count = 0
            while retry_count < max_retries:
                try:
                    tokens = tokenize(sentence)
                    if len(tokens) < 2 or len(tokens) + 2 > max_seq_len:
                        break
                    inp = [vocab["[BOS]"]] + tokens
                    tgt = tokens + [vocab["[EOS]"]]
                    X.append(inp)
                    Y.append(tgt)
                    lengths.append(len(inp))
                    break
                except Exception as e:
                    retry_count += 1
                    print(f"L·ªói khi tokenize (l·∫ßn th·ª≠ {retry_count}): {e}")
                    if retry_count < max_retries:
                        print("ƒêang th·ª≠ l·∫°i...")
                        time.sleep(1)
                    else:
                        print(f"B·ªè qua c√¢u: {sentence[:50]}...")
        time.sleep(0.1)
    return X, Y, lengths

# T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu
pretrain_data = load_pretrain_dataset(raw_dir / "pre_train.json")
X, Y, lengths = prepare_data(pretrain_data, vocab, max_seq_len, batch_size=50)

np.set_printoptions(threshold=np.inf)

# T·∫°o th∆∞ m·ª•c l∆∞u n·∫øu ch∆∞a c√≥
data_tokenized_dir.mkdir(parents=True, exist_ok=True)

# L∆∞u d·ªØ li·ªáu d∆∞·ªõi d·∫°ng npy
np.savez_compressed(
    data_tokenized_dir / "data_tokenized.npz",
    X=np.array(X, dtype=object),
    Y=np.array(Y, dtype=object),
    lengths=np.array(lengths)
)

print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu dynamic padding v√†o th∆∞ m·ª•c: {data_tokenized_dir}")
print(f"üìä T·ªïng s·ªë m·∫´u: {len(X)}")
print(f"üìà ƒê·ªô d√†i sequence trung b√¨nh: {np.mean(lengths):.2f}")
print(f"üìâ ƒê·ªô d√†i sequence min/max: {min(lengths)}/{max(lengths)}")

# G·ª£i √Ω t√≠ch h·ª£p EWC ƒë·ªÉ ngƒÉn catastrophic forgetting
"""
ƒê·ªÉ ngƒÉn catastrophic forgetting, c√≥ th·ªÉ t√≠ch h·ª£p Elastic Weight Consolidation (EWC) trong qu√° tr√¨nh hu·∫•n luy·ªán:
1. T√≠nh Fisher Information Matrix tr√™n d·ªØ li·ªáu g·ªôp ƒë·ªÉ x√°c ƒë·ªãnh c√°c tr·ªçng s·ªë quan tr·ªçng.
2. Th√™m penalty term v√†o h√†m m·∫•t m√°t:
   L = L_combined + Œª/2 * Œ£(F_i * (Œ∏_i - Œ∏_i_initial)^2)
3. V√≠ d·ª• code EWC trong TensorFlow:

class EWC:
    def __init__(self, model, dataset, lambda_ewc=1.0):
        self.model = model
        self.dataset = dataset
        self.lambda_ewc = lambda_ewc
        self.params = {var.name: var for var in model.trainable_variables}
        self.means = {var.name: tf.identity(var) for var in model.trainable_variables}
        self.precision_matrices = self.compute_fisher()

    def compute_fisher(self):
        precision_matrices = {name: tf.zeros_like(param) for name, param in self.params.items()}
        for x, y in self.dataset:
            with tf.GradientTape() as tape:
                logits = self.model(x)
                loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            for var, grad in zip(self.model.trainable_variables, gradients):
                precision_matrices[var.name] += tf.square(grad) / len(self.dataset)
        return precision_matrices

    def penalty(self, model):
        loss = 0
        for var in model.trainable_variables:
            name = var.name
            loss += tf.reduce_sum(self.precision_matrices[name] * tf.square(var - self.means[name]))
        return self.lambda_ewc * loss

# Hu·∫•n luy·ªán v·ªõi EWC
model = tf.keras.Sequential([...])
ewc = EWC(model, tf.data.Dataset.from_tensor_slices((combined_X, combined_Y)).batch(32))
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)
        loss += ewc.penalty(model)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

for epoch in range(num_epochs):
    for x_batch, y_batch in tf.data.Dataset.from_tensor_slices((combined_X, combined_Y)).batch(32):
        loss = train_step(x_batch, y_batch)
"""
