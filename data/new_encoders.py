from tokenizers import Tokenizer, trainers, models, pre_tokenizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence
from tokenizers.pre_tokenizers import Whitespace
import json
import numpy as np
from pathlib import Path

current_file = Path(__file__).resolve()
data_dir = current_file.parent
raw_dir = data_dir / "raw"
processed_dir = data_dir / "processed"
processed_dir.mkdir(parents=True, exist_ok=True)

# B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu
dataset = []
with open(raw_dir / "pre_train.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)
    dataset = [item.strip() for item in json_data if isinstance(item, str) and item.strip()]

# B∆∞·ªõc 2: T·∫°o tokenizer BPE
tokenizer = Tokenizer(models.BPE())
tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])
tokenizer.pre_tokenizer = Whitespace()
trainer = trainers.BpeTrainer(
    vocab_size=20000, min_frequency=2,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]", "[BOS]", "[EOS]"]
)
tokenizer.train_from_iterator(dataset, trainer=trainer)

# B∆∞·ªõc 3: L∆∞u tokenizer v√† vocab
tokenizer.save(str(processed_dir / "bpe_tokenizer.json"))

vocab = tokenizer.get_vocab()
sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])
with open(data_dir / "new_vocab.txt", 'w', encoding='utf-8') as f:
    for token, idx in sorted_vocab:
        f.write(f"{token}\t{idx}\n")

# B∆∞·ªõc 4: T·∫°o X, Y, lengths (gi·ªëng format BOS + tokens | tokens + EOS)
BOS_id = vocab.get("[BOS]", 0)
EOS_id = vocab.get("[EOS]", 1)

X, Y, lengths = [], [], []

for line in dataset:
    encoded = tokenizer.encode(line)
    token_ids = encoded.ids
    if len(token_ids) < 1:
        continue
    inp = [BOS_id] + token_ids
    tgt = token_ids + [EOS_id]
    X.append(inp)
    Y.append(tgt)
    lengths.append(len(inp))

# B∆∞·ªõc 5: L∆∞u to√†n b·ªô th√†nh 1 file .npz
np.savez_compressed(
    processed_dir / "new_data_tokenized.npz",
    X=np.array(X, dtype=object),
    Y=np.array(Y, dtype=object),
    lengths=np.array(lengths)
)

print("‚úÖ ƒê√£ l∆∞u X, Y, lengths v√†o: new_data_tokenized.npz")
print(f"üìä T·ªïng s·ªë m·∫´u: {len(X)} | ƒê·ªô d√†i TB: {np.mean(lengths):.2f}")
